{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "Data Collection - Pokemon and News Sites",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naked-yahoo"
      },
      "source": [
        "### Import `requests` library\n",
        "This package allows you to get any website's HTML code so that you can extract from it. Let's save the website's URL in the `URL` variable."
      ],
      "id": "naked-yahoo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acting-exhibit"
      },
      "source": [
        "import requests\n",
        "\n",
        "URL=\"https://bulbapedia.bulbagarden.net/wiki/List_of_Pok%C3%A9mon_by_National_Pok%C3%A9dex_number\""
      ],
      "id": "acting-exhibit",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "palestinian-feedback"
      },
      "source": [
        "### Load the page"
      ],
      "id": "palestinian-feedback"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fancy-representation"
      },
      "source": [
        "page=requests.get(URL)"
      ],
      "id": "fancy-representation",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featured-tenant"
      },
      "source": [
        "### Parse HTML data"
      ],
      "id": "featured-tenant"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skilled-ordinary"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "id": "skilled-ordinary",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liable-anger"
      },
      "source": [
        "### Find all tables that contain Pokemon details"
      ],
      "id": "liable-anger"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "returning-alcohol"
      },
      "source": [
        "# Get main content <div>\n",
        "poke_content=soup.find(id='mw-content-text')\n",
        "\n",
        "# Get all <table> elements\n",
        "poke_tables=poke_content.find_all('table')"
      ],
      "id": "returning-alcohol",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outer-progressive"
      },
      "source": [
        "### Save them in a JSON"
      ],
      "id": "outer-progressive"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minute-property"
      },
      "source": [
        "info_start = 3\n",
        "\n",
        "import json\n",
        "\n",
        "pokemon_json = []\n",
        "\n",
        "for j in range(1, 9):\n",
        "  gen_list = poke_tables[j]\n",
        "  gen_array = []\n",
        "\n",
        "  for i in range(info_start, len(gen_list.contents), 2):\n",
        "      poke_info=gen_list.contents[i]\n",
        "      kdex=poke_info.contents[1].text.strip()\n",
        "      ndex=poke_info.contents[3].text.strip()\n",
        "      name=poke_info.contents[7].text.strip()\n",
        "      type1=poke_info.contents[9].text.strip()\n",
        "\n",
        "      if len(poke_info.contents) > 10:\n",
        "          type2=poke_info.contents[11].text.strip()\n",
        "          gen_array.append({\n",
        "              \"kdex\": kdex,\n",
        "              \"ndex\": ndex,\n",
        "              \"name\": name,\n",
        "              \"type1\": type1,\n",
        "              \"type2\": type2\n",
        "          })\n",
        "      else:\n",
        "          gen_array.append({\n",
        "              \"kdex\": kdex,\n",
        "              \"ndex\": ndex,\n",
        "              \"name\": name,\n",
        "              \"type1\": type1\n",
        "          })\n",
        "\n",
        "  key_name = \"generation_\" + str(j)\n",
        "  pokemon_json.append({\n",
        "      key_name: gen_array\n",
        "  })\n",
        "\n",
        "with open('pokemon.json', 'w') as f:\n",
        "  json.dump(pokemon_json, f)"
      ],
      "id": "minute-property",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVyno2J4sYw"
      },
      "source": [],
      "id": "5eVyno2J4sYw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zygLSGxJ405t"
      },
      "source": [
        "# Getting news articles"
      ],
      "id": "zygLSGxJ405t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0W7pbg9405u"
      },
      "source": [
        "### Import `requests` library\n",
        "This package allows you to get any website's HTML code so that you can extract from it. Let's save the website's URL in the `URL` variable."
      ],
      "id": "c0W7pbg9405u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2_FEC2N405u"
      },
      "source": [
        "import requests\n",
        "\n",
        "URL=\"https://edition.cnn.com/article/sitemap-2021-3.html?fbclid=IwAR2FVSX87L5cGSae9RmGBw34kyVB-YHsTaFxpNdePuGqW5Du1kjD5j_SAi0\""
      ],
      "id": "R2_FEC2N405u",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkI-yXg2405v"
      },
      "source": [
        "### Load the page"
      ],
      "id": "YkI-yXg2405v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr4bvi4p405v"
      },
      "source": [
        "page=requests.get(URL)"
      ],
      "id": "pr4bvi4p405v",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO8oHeP6405w"
      },
      "source": [
        "### Parse HTML data"
      ],
      "id": "uO8oHeP6405w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZwgnRgSwKne"
      },
      "source": [],
      "id": "sZwgnRgSwKne"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDhnbmZ405x"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "id": "_EDhnbmZ405x",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Jw6NXg405x"
      },
      "source": [
        "### Find all news articles on March 11 and 12, 2021 and save it in a JSON file"
      ],
      "id": "A5Jw6NXg405x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWhbyKO405x"
      },
      "source": [
        "import json\n",
        "\n",
        "body=soup.find_all(class_ = 'sitemap-entry')\n",
        "news_articles = body[1].find_all('li')\n",
        "\n",
        "news_json = []\n",
        "\n",
        "for i in range(len(news_articles)):\n",
        "  articles = news_articles[i]\n",
        "  if articles.find(class_ = \"date\").text.strip() == '2021-03-12' or articles.find(class_ = \"date\").text.strip() == '2021-03-11':\n",
        "    article_url = articles.find('a')['href']\n",
        "    article_page = requests.get(article_url)\n",
        "    temp_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
        "    full_article = temp_soup.find(class_ = \"pg-rail-tall__wrapper\").find(class_ = \"pg-rail-tall__body\").find('section').find(class_ = \"l-container\").find_all(class_ = \"zn-body__paragraph\")\n",
        "    \n",
        "    full_article_string = ''\n",
        "\n",
        "    for article in full_article:\n",
        "      full_article_string = full_article_string + article.text.strip() + \"\\n\"\n",
        "\n",
        "    date = articles.find(class_ = \"date\").text.strip()\n",
        "    title = temp_soup.find(class_ = 'pg-headline').text.strip()\n",
        "    author = temp_soup.find(class_ = 'metadata').find('span').text.strip()\n",
        "    author = author.replace('By ', '')\n",
        "    author = author.replace(', CNN', '')\n",
        "\n",
        "    news_json.append({\n",
        "        \"title\": title,\n",
        "        \"author\": author,\n",
        "        \"date\": date,\n",
        "        \"full_article\": full_article_string\n",
        "    })\n",
        "\n",
        "with open('news.json', 'w') as f:\n",
        "  json.dump(news_json, f)"
      ],
      "id": "LsWhbyKO405x",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxhAaCTR405z"
      },
      "source": [],
      "id": "bxhAaCTR405z"
    }
  ]
}